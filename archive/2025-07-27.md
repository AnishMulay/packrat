prof raghvendra work

One of the things I have to do before Monday noon is that I have to set up some kind of research notes thing which I am doing right now. I have to get my research structured and in order before I send it to Professor Raghavendra because he has told me that I need to send him all the things that I have done in this week before Monday noon so he can review it before the meeting on Tuesday.


I found out the exact research paper to read, so I will be reading that. 

# the first research paper, Geometric Bipartite Matching based exact algorithms for server problems

I think it is important that we have mentioned the subsequences when getting like they mentioned K subsequences instead of K subsets because I think the order of the request is important. 

We have a sequence of requests made to K servers. We have to split the given sequence into K subsequences. For each subsequence, there is some kind of distance defined in the metric space of the request. I am assuming some kind of d-dimensional vector space where each request represents a d-dimensional point in that vector space. The cost of one subsequence is the distance between all consecutive pairs of that subsequence. This is the cost of one subsequence, and the total cost of all the subsequences is the total cost, which we have to minimize.

So, given a sequence of requests, we have to split it into K subsequences such that the sum of the costs of each of those is the minimum. And this will tell us the optimal way of serving those requests using K servers.

The offline k-server problem. The offline here means that we already know all of the requests which will come in. So we have the entire sequence of requests which will be arriving. I also talked to Claude the AI and it gave me a really nice idea or intuition of the SKPI problem. This SKPI problem is where we do have to divide the sequence into k sub-sequences but instead of only taking the cost of them (the total distance between consecutive pairs), we also have the servers starting at some initial location and then we add one more cost to it which will be the cost of the server moving from its initial location to the first request.

The really nice analogy it gave me was of robots where imagine there are some requests which need to be done and of course they need to be done in some kind of sequence. We have k robots and these k robots are in some k starting locations so we do need to factor in the cost of the robot moving to the first request as well. That's why my generalization is the KSPI problem. And the paper is saying that if we find the optimal way of doing this then that is the optimal solution for the General Offline K-Server problem. 

They define two more things in the introduction of the research paper:

1. Diameter
2. Closest pair

The diameter is defined as the farthest distance between any two request points, and the closest pair is defined as the closest distance (the smallest non-zero distance) between two request points. The ratio of these is denoted by Δ and they are calling it the spread.

When we are talking about Bipartite Matchings, then we have two sets A and B, and we need to find a vertex-disjoint set of edges. If these edges connect t points in A to t points in B, then this is the t-matching. If A and B are of the same size and the size is let's say N, then the t-matching becomes like perfect matching. And when A and B are both sets of points in a d-dimensional space, then we call it a geometric bipartite matching problem. The cost of a perfect match is the sum of all the edges, the sum of the weights of all the edges in the matching, and we have to minimize this cost, minimum cost geometric bipartite matching.

Okay, so I did get a bit confused on this. This research paper showed that the KSP problem can be reduced to the minimum cost bipartite matching problem. It didn't even show it; it just mentioned it. What the paper then does is it shows a reduction from the optimum (win-cost) bipartite matching to the KSP problem, and that's the part where I got slightly confused earlier. The thing with this is that you have two sets, A and B, both containing all of the requests (R1 to Rn). They are present in both set A and set B. Set A represents the starting points, and set B represents the ending points. The idea is that you find a t-cost matching if you want to find a matching if you have K servers. You find n-k because mod of A will be equal to mod of B, which will be n, because the requests populate A and B sets and they are the same.

And the idea is that set A shows the starting point and set B represents the ending point. So you find an n-k like the min cost bipartite matching of size n-k and this will also be the solution to the KSP problem. I'll just add a short conversation I had with Claude here. I've added it below. 

The next part of the paper talks about min-cost bipartite matchings. I understand min-cost bipartite matchings very well because I have done an entire course which talked about this. But what I am having trouble understanding next is that they have shown (they have stated a lemma) where someone proved that the KSP problem can be reduced to the min-cost bipartite matching problem. So if you can solve the KSP problem in some t(n) time, then you can solve the min-cost bipartite matching problem in t(n) + O(n^2) time. Okay, so maybe that's a lemma I will worry about that later. Next they have shown that there exists a reduction from the KSP problem to the min-cost bipartite matching problem. But they have tried to show a mean reduction from the min-cost bipartite matching problem to the KSP problem, and this is where I'm having some trouble understanding what they have written. I'll tell you what they have written, and I need you to understand it well. I know what min-cost bipartite matching is, and I understand that well. Please keep that in mind and help me brainstorm and fully figure out what they are trying to do with this reduction.

We strengthen the connection between the two problems by showing a reduction in the reverse direction, i.e., we reduce the k-SP (resp. k-SPI) problem to the minimum-cost bipartite t-matching problem. Given an input sequence σ of requests to the k-SP problem, we construct a bipartite graph Gσ with a vertex set A ∪ B and a set of edges E as follows. Vertex Set: For each request ri , we create a vertex bi (resp. ai) in B (resp. A) and designate it as the entry (resp. exit) gate for request ri . Edge Set: The exit gate ai of request ri is connected to the entry gate bj of every subsequent request rj with j > i with an edge. The cost of this edge is d(ai , bj ) = ∥ri − rj∥p. It is easy to see that a minimum-cost (n − k)-matching M in Gσ can be used to find an optimal solution to the k-SP problem. See Figure 1. For the k-SPI problem, for each server j, we add a vertex a j at the initial location sj to A and connect a j to the entry gate bi of every request ri in σ. The cost of this edge is d(a j , bi) = ∥sj −bi∥p. A minimum-cost n-matching in this graph can be converted to an optimal solution for the k-SPI problem. The formal proof of correctness for both these reductions is presented in Appendix A.1. A different reduction from k-SP and k-SPI problems to the minimum-cost flow problem has been presented in previous works [5, 25, 24], leading to the development of O(n 2k) time algorithm. However, unlike our reduction, their approach generates instances that include edges whose costs are −∞ and fails to maintain the metric properties of costs.

What I'm particularly having trouble understanding is what this exactly means. Like, what is the set A? What is the set B? How is a sub-sequence formed based on the matching? I don't understand why the sub-sequence thing comes in mainly.

Edit

Great question! This reduction is quite clever, and I can see why it's confusing. Let me break it down step by step, focusing on how the bipartite matching creates subsequences.

## Understanding the Vertex Sets

**Set A (Exit Gates)**: For each request rᵢ, there's a vertex aᵢ representing the "exit" from that request.

**Set B (Entry Gates)**: For each request rᵢ, there's a vertex bᵢ representing the "entry" to that request.

So if you have requests r₁, r₂, r₃, r₄:

- A = {a₁, a₂, a₃, a₄} (exit gates)
- B = {b₁, b₂, b₃, b₄} (entry gates)

## The Key Insight: How Matching Creates Subsequences

Here's the crucial part: **An edge from aᵢ to bⱼ (where j > i) represents a server moving directly from request rᵢ to request rⱼ**.

When you find a minimum-cost (n-k)-matching:

- You're selecting (n-k) edges
- Each selected edge aᵢ → bⱼ means "after serving request rᵢ, go directly to request rⱼ"
- The unmatched vertices indicate "breaks" in the sequences

## Example Walkthrough

Say you have 4 requests and want k=2 servers (so you need a (4-2)=2-matching):

**Possible matching**: {a₁→b₃, a₂→b₄}

This matching means:

- After serving r₁, go to r₃
- After serving r₂, go to r₄
- a₃ and a₄ are unmatched (sequence endpoints)
- b₁ and b₂ are unmatched (sequence starting points)

**Resulting subsequences**:

- Server 1: r₁ → r₃ (because a₁→b₃ is in matching)
- Server 2: r₂ → r₄ (because a₂→b₄ is in matching)

The unmatched entry gates (b₁, b₂) tell you where sequences start, and unmatched exit gates (a₃, a₄) tell you where sequences end.

## Why (n-k)-matching?

With n requests and k servers:

- You need exactly k subsequences
- This means k "starting points" and k "ending points"
- So you have k unmatched vertices in A and k unmatched vertices in B
- Total unmatched vertices: 2k
- Total matched vertices: 2n - 2k
- Number of edges in matching: (2n - 2k)/2 = n - k

## For k-SPI Problem

They add server vertices a^j (with initial locations) to set A, and these can connect to any entry gate bᵢ. Now you need an n-matching because you have n+k total vertices in A and n vertices in B, and you want n edges to assign each request to exactly one server.

Does this help clarify how the matching structure creates the subsequences?

Yeah, so this above is the conversation that I had with Claude. 

One interesting point they talk about is related work. They mention a couple of related research, in which they said that Gattani showed an algorithm where, in expectation, the min cost bipartite matching can be found in a certain way. But what they've mentioned is that when you use the Hungarian algorithm, which is a greedy algorithm in some sense, you also find the min cost T matching (where T is something between 0 and N). The GRS algorithm that Gattani suggested does not exhibit this property, so you can't find a T cost minimum or min cost T matching with this algorithm. 

The main contribution of this paper is that instead of making a quadratic number of queries to the dynamically weighted Nearest Neighbors data structure. What you can essentially do is you can find a solution to the KSP and KSPI problems by finding a N-K matching on a min cost N-K matching in the appropriate graph, and you can do this by creating a structure by doing a sub-quaternary number of queries to the DWNN data structures.

- The L1 norm is usually the Manhattan distance.
- The L2 norm is usually the Euclidean distance.
- The L3 norm is the Chebyshev distance.

The extended matching concept that we talk about in the research paper basically means that after the space has been split up into rectangles, the matching inside one rectangle is an extended matching because it can match with the boundaries as well. This is based on the original paper that Professor Raghavendra shared with me. 

Professor Raghavendra has told me to find AI and Machine Learning algorithms, so I think primarily Machine Learning algorithms that I can use.

One of the main things I have to do is find out what Machine Learning algorithms can be used. For any Machine Learning algorithm, it tries to predict something, and I think I am not fully sure, but based on me reading the paper right now, it seems that for the offline K-server problem, there is a solution, and the K-server problem is very similar to the New York taxi problem that Professor Raghavendra was talking about.

The idea is that you train an algorithm on the optimal solutions of like the previously computed optimal solutions. And because this is offline, like the optimal solution can only be calculated offline once you know all the requests in advance. I think what Professor Raghavendra wants to do is to find a Machine Learning algorithm which with high accuracy can predict something, and I think the main goal I need to figure out right now is what is this something? That machine learning algorithm will try and predict. 

The dual framework is based on dividing a rectangle and the way it's done is as follows:

First of all, we only divide into rectangles who have an aspect ratio of smaller than or equal to t, which means that the rectangles are reasonably squarish.

Now, when we want to divide one rectangle into two different rectangles, here is what we do:

Firstly, we need to find a divider line. This divider needs to be like between one-third and two-thirds of the rectangle's side. If I'm dividing by length, it's the same as it is for height. But for now, if I'm dividing by length, then it is between 1/3 and 2/3 of that side. So the divider lies there, and also that the divider has the minimum number of points closed to it. 

Here is what is written exactly in the paper. "We Partition a rectangle into two children by picking a divider (the purple dashed vertical line) with the minimum number of points close to it (gray shaded area) within the middle-third of its longer side (the part between the two vertical dotted lines)."

Section 2.1, the Hierarchical Partitioning section of the research paper, defines what it means when you say that you want to choose a divider in the middle third of the rectangle such that the points closest to it are minimum. It defines that in a mathematically rigorous way. 

So, Extended Bipartite Matching म्हणजे काय? ते C Cells मारे डिवाइड करतात recursively आणि ते recursively डिवाइड करतात, Until like number of points is smaller than or equal to 2. 

आणि त्या सल तो सेल केला, तो node leaf node होतो. 

आणि असे सगे-सगे-सगे divide करून, you can have a set of C cells, ज्यांनी अखेर original square cover होता होता आहे. 

आणि यामध्ये ते एक नवीन प्रकारचा matching define करतात, ज्यामध्ये मुख्य प्रश्न म्हणजे match करणे, किंवा एक point B मधील point ला match करणे किंवा boundary point ला match करणे. 

आणि मग आपण boundary साठी distance define करतो, जो एक निश्चित metric आहे. 

एक दिवस boundary आणि त्या point च्या distance चा विचार केला, तर boundary च्या त्या cell च्या boundary पर्यंतचा distance असतो. 

आणि अधिक under the rush, मी LJ boundary चा विचार केला, म्हणजे ते अजून extended matching च्या minimum cost matching चा विचार करतात. 

आणि boundary च्या match current allowable चा विचार करतो, मी कोणत्याही वेळेस define करू शकतो. 

Section 2.3 talks about a Constrained Dual Framework. In this framework, they define what are Feasible Weights and what is an Admissible Path, which will later be used to find the Minimum Cost Matching inside that cell. They also define the admissible weights or the feasible weights for points which are matched to the boundary. For these points, they will then define an upper bound which is O(4/5). So they are saying that after we do this, there will be at most O(n^4*5) points which are matched to the boundary. This basically means that as we like merge these rectangles together, we would need to match these many points later on.

The actual algorithm which they have defined is for the K-SP problem in two dimensions and it has two major steps:

1. The Extended Hungarian search step, in which they use a priority queue
2. The merge step, where they rub the boundary between two rectangles or two nodes and then merge them and then find or update the minimum cost matching

Section 4 is all about faster implementations of the Hungarian algorithm specifically for the K-SP and K-SPI problems. 
