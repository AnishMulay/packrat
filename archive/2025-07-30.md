# Professor Raghvendras work

I think one of the things which is obvious is that a lot of machine learning algorithms might not work as well for 100,000 taxis. So the first thing which comes to mind is that we need to narrow this down to a smaller number. Even though we have the global optimal solution, we can try and work with a smaller subset. Like in the paper provided by Professor Raghvendra, we can divide the city into cells of some kind and maybe work with those. 

We could try a few reinforcement learning algorithms since we have a lot of data. We can use that data to train early like some kind of reinforcement learning agent which kind of tells each taxi where to go. It would be interesting to know what the state space in that case would be.

I think one of the problems here might be that the underlying problem or the underlying thing is changing over time. It's not constant, so reinforcement learning is usually used for cases where the underlying thing is constant, like a game or something like that. So like AlphaGo or it's used for some kind of computer games where the demand is constant.

Maybe I can model the New York taxi thing as a game in some form. Maybe I can add some kind of visual representation to this as well so that's an interesting angle to explore. So reinforcement learning is like one zone of machine learning algorithms which we can use. 

Apart from reinforcement learning, the other kind of thing we could try is training a value network. A value network is something which in real-time will kind of try to predict a probability distribution over the city for a particular taxi. Yeah, that's something I had in mind as well.

There is also something called Imitation learning which I found a research paper for, and they have tried that. That is something we can try as well. In this case, we kind of train a machine learning model by using already known optimal solutions for the offline problem, presenting it as an online problem, and then training a machine learning network model to predict the best decision to make. 

# Sandstore Project Raft Consensus Algorithm.

A replicated log is a data structure where multiple servers try to maintain the same log, which means the same events in the same order, and they try to replicate this across multiple machines. I like the analogy of it being like multiple people trying to keep their notebooks in sync.

There are three important points to a replicated log data structure:

1. Ordering: If a certain order of events is observed in this log of one server, it should be the same across all the servers.
2. Consistency: Consistency states that eventually (because we would usually operate on an eventual consistency model), all the servers will have the same log entries and in the same order, or in short, their logs will be in sync.
3. Durability: Which means that once a log entry has been committed (committed here means it has been acknowledged by a majority of the servers), then it cannot be removed.
   
In my Sandstore project right now, the push-based metadata replicator has a lot of problems. One of the ways in which this problem, like the problem of metadata replication, can be solved is that instead of handling the metadata application operations yourself or rather in the application code, what I do is I will submit the log entries (e.g. "create file metadata for this" and then "update file metadata") and the Raft consensus algorithm will take care of the rest. I think this kind of fits neatly into my current metadata replicator interface, but I will have to look at the additional functions that the metadata replicator interface might need to offer later on. 

The other thing which I think is conducive is that once Raft commits the transaction in the log, or whatever operation we have executed (like creation of metadata or updation of metadata), then all the servers will actually do it in their metadata store. So if I have like an in-memory metadata currently, I have an in-memory metadata service, and this in-memory metadata service can be used because currently in the push-based replication, I am making the update in the in-memory (whatever it's an interface at the end of the day). But currently, I have only implemented in-memory, so it makes the memory thing and then it pushes the metadata to the other servers, but of course this has many problems. So if I use Raft, what I think will happen is I will submit an event or submit an operation to the Raft consensus whatever thingy, and once it commits that operation into the log, then I can update my metadata store, and this will kind of guarantee that all the servers have the same metadata. 

The reason the researchers wrote the Raft paper, "In Search of an Understandable Consensus Algorithm," is because the algorithm used for consensus before that, which was PAXOS, was very difficult to understand. If you wanted to integrate it into your current system, you would need to do a lot of complex changes to your system. 

The primary goal of the authors in writing this paper or creating this consensus algorithm was understandability. How easy is it to understand this algorithm compared to other consensus algorithms.

Raft separates leader election, log replication, and safety. 

One of the things which makes Raft easier to understand is that it uses a stronger form of leadership. So logs only flow from the leader to the other servers, which makes it slightly easier to understand. 

I think I need to understand the concept of membership changes. 

Membership changes mean that servers can move in and out of the cluster, but I may need to understand it in more detail. 

I will need to understand replicated state machines, and for that, I first need to understand what state machines are. State machines are machines that can exist in only one state at a time and then it can transition between states. It is important to note that state machines are deterministic in nature, not stochastic. And the transition between states happens based on the input given.

Okay, so we can think of one single file system or one single server as a state machine, and the state machine has a particular state (current state) which we can think of as the metadata of all the files. You can receive that you can give inputs like "store file" or "update file", and I am reasonably sure that read commands don't affect their states, so they don't count as valid input (they do count as valid inputs, but they won't actually change the metadata of a particular file). And then you have things like "delete" as well, so the state of this single file server state machine changes as the input is given. It is very important to understand what a replicated State Machine is. A replicated State Machine means the same state machine is running on multiple different servers and we need it to be synchronized. Given the deterministic nature of State Machines, if the same input is provided to the servers in the same order, then the State Machines on those servers will be synchronized - by virtue of the determinism of State Machines.

What the replicated log does here is it helps us make sure that each state machine on each server receives the input inputs in the same order, so we guarantee that the inputs are given in the same order and the same inputs are given, of course, and hence we guarantee that the state of the State Machine is replicated correctly. 

There is a really nice separation of concerns here which works well with my Sandstore project. The separation of concerns here is that the difficult part about the distributed log replication is handled by Raft, and then the changes that need to be made on the local server can be handled by that interface, which is the Metadata Replicator interface. This is essentially what gives it a nice structure because I've made the project in Go and used a lot of interfaces, I think it works very well with my project. 

There will be a consensus module here, and this consensus module I think, so raft, the Raft paper mentions consensus modules, and I think in my case, the consensus module will be a part of the better data replicator itself. I might add another interface for the type of consensus algorithm, but I think because they are so vastly different in their implementation, it's better to leave the interface at the metadata replicator level. 

Consensus algorithms for practical systems typically have the following properties: 
• They ensure safety (never returning an incorrect result) under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering. 
• They are fully functional (available) as long as any majority of the servers are operational and can communicate with each other and with clients. Thus, a typical cluster of five servers can tolerate the failure of any two servers. Servers are assumed to fail by stopping; they may later recover from state on stable storage and rejoin the cluster. 
• They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems. 
• In the common case, a command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.

In the research paper, the authors say that in order to understand PAXOS, they had to read multiple research papers and articles which showed simpler versions of PAXOS. They had to try and implement their own version of the PAXOS protocol in order to understand it better, which took almost a year. I think this is one of the reasons they tried developing their own.

One of the things I've realized is that the PAXOS protocol was written in a way that the consensus would be for one thing, like one log entry, and then multi-PAXOS would kind of build this log thing where you have multiple iterations or a completely different algorithm based on PAXOS for generating a log. The authors of the Raft paper, however, thought that it was a better idea to base the consensus algorithm and design it around a log itself. I think this is a better way to do it anyway. The authors have really put it in a nice way.

Another problem that the researchers mentioned with PAXOS is that it is very theoretical and it is focused on proving its correctness. Hence, its actual real-world implementations are far from the actual PAXOS protocol, which makes it more difficult. So before the Raft thing came out, usually when people need to implement consensus correctly, they started with PAXOS and then realized the problems with it and eventually delved into their own niche implementations. 

When designing the Raft consensus algorithm, the authors of the paper stated that they focused on understandability from the start. One of the things that helped them was decomposition. Whenever they faced a decision on what to choose in the design process, they chose the one which could be decomposed into smaller parts, explained well, and also explained well how these parts could be combined to form the final solution. In the case of Raft, this became data reduction, log replication, and membership changes. One other thing they tried to do was reduce the state space - by this, I think they mean that the number of possible states that the replicated state machine can be in. They also mentioned in the paper that they tried to reduce the number of states and also they tried to reduce the non-determinism in general. But in some cases, like they mentioned in leader election, the non-determinism actually helped improve the understandability of the algorithm.

One thing I might have to consider if I implement Raft is that I might have to make some changes which make it so that the client can't send requests to any of the existing servers but it has to send it to the leader. Or, the Raft algorithm, the communication layer, or the node registry takes the responsibility of routing the request to the leader.

I will also have to think about how Raft fits in with the other parts, like eventual consistency, and stuff like that. Basically, like synchronous, asynchronous, and replication.

One of the fundamental ideas of Raft is that it works based on leader election. Leaders are elected for every term. Each leader has a term for which it is a leader, and this simplifies log replication because the leader basically decides the log and which entries to append and in which order. So the order in which the clients send entries or the order in which the leader decides to append entries to the log becomes a signal source of truth type order and then it is replicated like it goes downstream to the followers. If a leader fails then a new leader is elected.

Then it splits this larger problem of having a replicated log into three smaller, somewhat non-overlapping—though not entirely—subproblems. The first one is Leader Election. A leader is elected whenever the current leader fails, or at the beginning of a new term. It then becomes the leader's responsibility to maintain its log. Then there is the a property of log replication where the leader ensures that the way it sees the entries (rather than your entries in the log) is the same way all of the other servers in the cluster see these entries .And finally, your safety property is maintained, which basically means that each entry in the log has a particular index, and if one (any) of the servers in the cluster has an entry at a particular index, then all other servers must have exactly that entry at that index. They cannot have some other command or entry at that index.

A Raft cluster contains multiple servers, a typical number is five servers. A raft cluster can tolerate the failure of a number of nodes which is not a majority. So if n < n/2.

The Raft paper does mention how a particular server reacts if a client sends it a request. The Raft paper says that if the server is a leader, it just processes the request, but if the server is a follower, then it redirects the request to the leader.

One of the problems is that there will be a tight coupling between the consensus algorithm or metadata replicator and the server. I will have to write a new server. I will basically have to write a new server, and I think that the reason for that is this redirection from the follower to the leader. I might try to find a way around this, but let's see what we can do. Maybe that tight coupling is valid. I don't.

Raft divides time into two terms. These terms are of arbitrary length. At the start of each term, a leader election happens, and during the leader election, each server becomes a candidate. Then after the election ends, there will be only one single leader, and this leader will be for the entire term.

Terms act as a logical clock and successive terms are numbered with consecutive positive integers. Okay, I think the way the current term is handled is very nice and simple. Each server has a current term, which it believes to be the current term. When it communicates with another server, both servers share what they think the current term is, and then they default to the larger one. This is because that is the term which is most likely happening right now. The server with the older current term number will essentially figure out that it is using an outdated term number and can update it.

The above thing of the current term number was for followers. If a leader realizes that, like it is seeing, if a leader sees a current term number which is higher than its own, then it immediately defaults to a follower state. And if a server receives a request with a stale term number, then it rejects that request.

The Raft consensus algorithm in general seems to rely on RPC. This might be another bottleneck along with the server where the communicator cannot be anything that we want it to be. A message because in my case, the actual exchanging of the messages between servers is handled by the communicator. I can definitely create a separate Raft communicator, but we would need to see what kind of drawbacks come with that because it will definitely be bad for the overall idea of the learnability of the project because it kind of violates the idea that the communicator is the only thing which is used for communication. If the communicator is selected as HTTP for example, then does that mean that all communication with everything happens with HTTP or does that mean that communication with client happens in HTTP? That is something I have to think about later.

Leaders periodically send a heartbeat. The heartbeat is basically an RPC message of type "append entries," but without any log. This acts like a heartbeat, and if a server is in the follower state and it keeps receiving this heartbeat, then it knows that the current term is ongoing and there isn't an election right now. If a follower does not receive a heartbeat within a specified period of time, which is called the leader election timeout, then it assumes that there is some problem. And then starts an election to elect a new leader.

Election timeouts are chosen randomly within a fixed interval of 150ms to 300ms. Section 5.2 in the Raft paper talks in detail about exactly how the leader election works.

There seems to be a concept of a log for the leader. The idea is for log replication when the leader receives a request from the client, it first adds it to its log, then it sends an append entry RPC message is sent to all the servers once the log has been safely replicated. Then, the leader performs whatever action the log entry is talking about (e.g., create file, update file, delete file). It creates this, it actually does the job that is entry specified on the leader server.

The leader decides when it is safe to apply a particular entry to the replicated log or the replicated state machine. This is called a commit. It is said that the entry is committed. Once a follower knows that an entry is committed, it applies that entry to its local state machine.

When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries. If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.

The Append Entries RPC message doesn't just contain multiple log entries that the leader is telling the follower to append. It is not necessary that it's just zero log entries or one log entry.

The crashing or detecting that a node has failed and stuff like that is not separate from Raft, but it is actually a part of Raft itself. This kind of ties the node registry into a tight coupling. So, I will either update some kind of in-memory structure which maintains the current node registry, and then have the Raft metadata replicator make changes to this node registry, which the node registry interface can then pull from, or something else like that.

So, there might be a case. I have mentioned in Section 5.3 that there might be a case where once a new leader is elected, it might override the committed logs of the previous leader, which will violate some properties, especially the durability property. This property states that once a log entry is committed, it's durable across all servers. The way it is done is that Raft ensures that whenever a new leader is chosen, it contains all of the committed entries of the previous leader.

I think one of the most important points that I've realized is that the raft cluster never forgets its original size. So even if a few nodes crash, it has a certain size. The entire cluster is a certain size. And when we talk about leader election or log replication before commitment, and when we see majority, the majority is in terms of the original size of the raft cluster, not the size after the crash. This caused some confusion with me before.

# Sandstore project

Next up in the Hansdorp project, I'm going to be incorporating the Raft consensus algorithm. Right now, there is one interesting thing that I learned in my Communicator interface: the `communicator.send` function is non-blocking because HTTP and gRPC are both blocking. But in order for the heartbeat and voting messages to work correctly in the context of Raft, I need `communicator.send` to be non-blocking. The way it has told me to solve this is by wrapping it around like wrapping it inside a go func and then doing that. This will help me to handle the response whenever it comes and update my thing accordingly, and it is non-blocking, so it won't block up the communicator channel as well. So I think this is very nice, it can be added to the personal stack overflow Apart from obviously adding it to the sand store notes.
